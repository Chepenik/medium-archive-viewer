{
  "id": "6a7c2f7055ea",
  "title": "Day 1381",
  "createdAt": "2026-01-02 03:53:20",
  "tags": [
    "ai",
    "is",
    "simply",
    "a",
    "powerful-tool"
  ],
  "url": "https://chepenikconor.medium.com/day-1381-6a7c2f7055ea",
  "content": "# Day 1381\n\nToday was a holiday, which meant no work and a whole lot of time to mess around with one of my favorite ai tools: Claude Code. It's kind of ridiculous how powerful it already is, and also how obvious it is that I'm still only using a small slice of what it can do. There are so many commands, so many plugins, so many workflows I can tell exist but haven't really clicked into yet. I started watching a course on it and within the first stretch I already had a handful of \"ohhhh... that's how you can actually leverage this\" moments. And it instantly sends your mind forward. Like, what does this look like by 2027? Is it still a terminal-first tool, or does it become something closer to a true interface layer for building? Will we treat it like Jarvis from Iron Man and stop having to touch the command line? You just say: \"Hey Claude Code, I want a million-dollar ARR SaaS idea. Build it. Integrate Stripe. Add Bitcoin payments as well. Go ahead & register the LLC. Don't mess anything up. Thank you!\" It sounds absurd, but the speed of exponential curves makes it hard to laugh off the idea completely.\n\nBut the most interesting thing I ran into today wasn't a new command or a slick integration. It was [a podcast](https://www.youtube.com/watch?v=8JJs4-pWcm8) that challenged how I've been thinking about LLMs in the first place, and it reframed the \"wow\" feeling into some fair skepticism. The guest, David Dredge, is a risk guy. You know long volatility, long convexity, the exact kind of brain that looks at a system and asks where it breaks, not what it does on a normal day. His core framing is simple and brutal: risk isn't what you think is going to happen. Risk is what hurts if it happens. A lightning strike that burns one tree is meaningless. The disaster is the dry brush, the buildup, a.k.a the hidden fragility that turns a small spark into a forest fire. And in markets, he argues that the dry brush is leverage, especially the kind that accumulates quietly under \"safe\" assumptions and bad models, because people confuse volatility with risk and then build entire institutions around that confusion. I mean seriously \"tHe RiSk FrEe RAtE\". Come on now.....\n\nThis maps onto Bitcoin in a way most TradFi conversations refuse to. Dredge makes this distinction between \"naturally volatile\" assets and \"artificially suppressed volatility\" assets. In his framing, the most dangerous stuff in the world isn't the thing that swings around in public where everyone can see it. The dangerous stuff is the thing that looks calm, gets labeled safe, attracts leverage, and then blows up when the tail event finally shows up. That's the forest floor stacked with brush. That's the system that rewards people for selling volatility, clipping yield, and pretending the downside doesn't exist... until it does because you can only centrally plan something for so long.\n\nThen the conversation pivots into the part that really mattered to me: how TradFi getting its \"dirty little fingers\" into Bitcoin changes the risk profile, not because Bitcoin's fundamentals change, but because the structure around it changes. ETFs, options on ETFs, structured products, auto-callables, knock-in puts... all the machinery that turns markets into reflexive magnets where price gets pulled toward pain points because undercapitalized positions have to be hedged, rolled, unwound, or rescued. He describes this \"max pain\" phenomenon as something real, not meme magic: if there's a concentrated vulnerability in positioning, the market can get dragged toward it like gravity because the hedging flow becomes the event. In other words, the lightning strike isn't the story. The story is how much brush was already piled up where the fire would spread.\n\nAnd then he drops what was be the best line in the whole episode: \"Positioning is the only thing that matters.\" Not narratives, not predictions, not vibes: positioning. Who is levered, where, and what happens when they're forced to move. That alone is a really clean lens to hold up to the whole \"Wall Street is here\" chapter of Bitcoin. Maybe the Trojan horse isn't only about getting Bitcoin into their system. Maybe it's also about their system pouring itself into Bitcoin, bringing along all the incentives, all the leverage games, all the accounting tricks, all the \"we get paid each lap\" behavior that Dredge argues basically guarantees periodic crisis.\n\nAnd that's where the podcast elegantly clips into something I had not been able to articulate in my own head: humans know more than they can describe, and LLMs can describe more than they know.\n\nDredge references Michael Polanyi and the idea of tacit knowledge, the stuff you can do without being able to fully explain it. You can ride a bike, but you can't necessarily write out the equations of balance and motion that make it work. You learn it through consequence, through feedback, through actual \"skin in the game.\" Then he flips it into what he calls the reverse Polanyi paradox for LLMs: AI can tell more than it can know. It will answer every question with confidence, but it doesn't understand anything in the way humans mean by understanding. It predicts the next word. It compresses a huge corpus into something usable because at the end of the day it's a tool. An insanely powerful one. But it can sound like wisdom without actually carrying the weight of lived consequence behind it.\n\nThat resonated with me, because it explains the exact tension I feel when I use these tools all day. On one hand, Claude Code (and tools like it) make you feel like you're holding fire. You can move so fast it's intoxicating. On the other hand, speed can trick you into thinking the machine \"knows.\" And maybe the real edge, going forward, is not becoming the guy who types prompts the fastest , but rather it's becoming the person who can separate fluent outputs from grounded truth. The person who knows where the brush is. The person who knows what's unrecoverable.\n\nWhich brings me to the line I keep replaying: \"Do everything you can to eliminate the unrecoverable so that you can pursue the unimaginable.\" That's risk management, sure. But it's also a philosophy for building with AI. The dream isn't \"let the tool do everything.\" The dream is protect yourself from the mistakes you can't come back from, so you can take bigger swings everywhere else. Good brakes on the race car so you can drive faster. Guardrails around the parts that could wreck you such as security, custody, permissions, capital, reputation, etc so you can experiment hard on the parts that can't kill you.\n\nSo yeah, today I played around with Claude Code and got that familiar \"this is the future\" feeling. But the podcast added something better than hype. It added a constraint. A reminder that fluency isn't knowledge, that volatility isn't risk, and that the systems that look safest are often just the ones with the most leverage hiding inside them. If AI really does keep accelerating, the people who win won't be the ones who outsource thinking. They'll be the ones who keep their judgment sharp, cut off the unrecoverable, and use these tools to chase the unimaginable without letting the forest burn down around them.\n\n![[https://x.com/ConorChepenik/status/2006920320264065256?s=20](https://x.com/ConorChepenik/status/2006920320264065256?s=20)](https://miro.medium.com/1*4G_qXxqNZ_WFz7Wq_rxpEw.png)\n\n1/1/26\n\nConor Jay Chepenik",
  "wordCount": 1233,
  "readingTime": 4.8528301886792455,
  "claps": 0,
  "voters": 0
}