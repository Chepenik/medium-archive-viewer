{
  "id": "85d35000f800",
  "title": "Day 1120",
  "createdAt": "2025-04-16 03:50:01",
  "tags": [
    "building",
    "archiving",
    "swan",
    "ai",
    "coding"
  ],
  "url": "https://chepenikconor.medium.com/day-1120-85d35000f800",
  "content": "# Day 1120\n\n![An AI generated Swan](https://miro.medium.com/1*9NrQ3s1phwcQGz2G5VCeXg.png)\n\nToday the company I work for and am a proud equity owner in became what some in the business call \"an AI first company\". Honestly, I am so glad to see this because the companies that don't embrace AI are missing out. The tools are getting better every single day it feels like.\n\nAnother thing I'm proud of is to post on Medium every day. But...it's also wild to think one place has all this data and if something happened to their servers, God forbid, it would sadden me because all of the writing I've done on here would be *poof* gone.\n\nSo I said you know what let me do some backing up just in case.\n\nWell with the help of AI (And the [unofficial mediumAPI](https://mediumapi.com/documentation.html)) we got about 500 articles now backed up in JSON format. Shout out the person who made that API by the way, extremely helpful.\n\nSo I had my AI in Cursor write me a lot of code including a front end. Then this main script below did the heavy lifting and helped me archive around 500 of my articles tonight.\n\n```javascript\nrequire('dotenv').config();\nconst axios = require('axios');\nconst fs = require('fs');\nconst path = require('path');\n\nif (!process.env.MEDIUM_API_KEY) {\n  console.error('Error: MEDIUM_API_KEY is not set in .env file');\n  process.exit(1);\n}\n\nif (!process.env.MEDIUM_USERNAME) {\n  console.error('Error: MEDIUM_USERNAME is not set in .env file');\n  process.exit(1);\n}\n\nconst HEADERS = {\n  'x-rapidapi-key': process.env.MEDIUM_API_KEY,\n  'x-rapidapi-host': 'medium2.p.rapidapi.com'\n};\n\nconst baseURL = 'https://medium2.p.rapidapi.com';\nconst dataDir = path.join(__dirname, 'medium_archive');\nif (!fs.existsSync(dataDir)) fs.mkdirSync(dataDir);\n\nasync function getUserId(username) {\n  try {\n    console.log(`Fetching user ID for: ${username}`);\n    const url = `${baseURL}/user/id_for/${username}`;\n    const res = await axios.get(url, { headers: HEADERS });\n    console.log('User ID response:', res.data);\n    if (!res.data.id) {\n      throw new Error('No user ID found in response');\n    }\n    return res.data.id;\n  } catch (error) {\n    console.error('Error fetching user ID:', error.response?.data || error.message);\n    throw error;\n  }\n}\n\nasync function getAllArticles(userId) {\n  try {\n    console.log(`Fetching all articles for user ID: ${userId}`);\n    let url = `${baseURL}/user/${userId}/articles`;\n    let allIds = new Set();\n    let retryCount = 0;\n    const maxRetries = 3;\n    \n    while (url && retryCount < maxRetries) {\n      try {\n        const res = await axios.get(url, { headers: HEADERS });\n        const articleIds = res.data.associated_articles || [];\n        console.log(`Got batch of ${articleIds.length} articles`);\n        \n        // Add new articles to set to avoid duplicates\n        articleIds.forEach(id => allIds.add(id));\n        \n        // If we have the next token and haven't reached our target, continue\n        if (res.data.next && allIds.size < 50) {\n          url = `${baseURL}/user/${userId}/articles?next=${res.data.next}`;\n          // Add delay between requests\n          await new Promise(resolve => setTimeout(resolve, 2000));\n        } else {\n          url = null;\n        }\n      } catch (error) {\n        console.error('Error in pagination:', error.message);\n        retryCount++;\n        await new Promise(resolve => setTimeout(resolve, 5000));\n      }\n    }\n    \n    console.log(`Total unique articles found: ${allIds.size}`);\n    return Array.from(allIds);\n  } catch (error) {\n    console.error('Error fetching all articles:', error.response?.data || error.message);\n    return [];\n  }\n}\n\nasync function getArticleData(articleId) {\n  try {\n    console.log(`Fetching data for article: ${articleId}`);\n    const infoUrl = `${baseURL}/article/${articleId}`;\n    const contentUrl = `${baseURL}/article/${articleId}/markdown`;\n\n    const [infoRes, contentRes] = await Promise.all([\n      axios.get(infoUrl, { headers: HEADERS }),\n      axios.get(contentUrl, { headers: HEADERS }),\n    ]);\n\n    return {\n      id: articleId,\n      title: infoRes.data.title,\n      createdAt: infoRes.data.published_at || infoRes.data.createdAt,\n      tags: infoRes.data.tags || [],\n      url: infoRes.data.url,\n      content: contentRes.data.markdown,\n      wordCount: infoRes.data.word_count,\n      readingTime: infoRes.data.reading_time,\n      claps: infoRes.data.claps,\n      voters: infoRes.data.voters\n    };\n  } catch (error) {\n    console.error(`Error fetching article data for ${articleId}:`, error.response?.data || error.message);\n    throw error;\n  }\n}\n\nasync function archiveMediumPosts() {\n  try {\n    console.log('Starting Medium archive process...');\n    const userId = await getUserId(process.env.MEDIUM_USERNAME);\n    console.log('Got user ID:', userId);\n    \n    // Get all articles\n    const articleIds = await getAllArticles(userId);\n    \n    if (articleIds.length === 0) {\n      throw new Error('No articles found for this user');\n    }\n\n    console.log(`Found ${articleIds.length} articles to process`);\n    \n    for (let id of articleIds) {\n      const filepath = path.join(dataDir, `${id}.json`);\n      if (fs.existsSync(filepath)) {\n        console.log(`Skipping existing article ${id}`);\n        continue;\n      }\n      \n      try {\n        const article = await getArticleData(id);\n        fs.writeFileSync(filepath, JSON.stringify(article, null, 2));\n        console.log(`Saved article: ${article.title}`);\n        \n        // Add a longer delay between requests to avoid rate limiting\n        await new Promise(resolve => setTimeout(resolve, 2000));\n      } catch (error) {\n        console.error(`Failed to save article ${id}:`, error.message);\n        // Add retry delay if we hit an error\n        await new Promise(resolve => setTimeout(resolve, 5000));\n      }\n    }\n    \n    console.log('Archive process complete!');\n  } catch (error) {\n    console.error('Archive process failed:', error.message);\n  }\n}\n\narchiveMediumPosts().catch(console.error);\n```\n\n![Important disclaimer: [https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2](https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2)](https://miro.medium.com/1*tDSc4FI9j68KhrrAFHGk1w.png)\n\nSo yeah super helpful what computers can do in regards to automating stuff. Feels good to run scripts, and build them with AI. I can not get over what an exciting time for builders in the space. Here is what the front end looks like of the site I built.\n\n![[https://github.com/Chepenik/medium-archive-viewer](https://github.com/Chepenik/medium-archive-viewer)](https://miro.medium.com/1*5Ub8L5hHGQDf7ndp_W3MWA.png)\n\nI still need to tighten up a few things for the github repo but holy smokes. This would have taken me like 2 weeks to do all of this before AI... maybe even longer ðŸ˜‚  Now I can do it in one hour with some solid prompts. I'm excited to see the products, goods, and services people will build with these tools.\n\nI am also excited to build my own. Future looks bright :)\n\n4/15/25\n\nConor Jay Chepenik",
  "wordCount": 808,
  "readingTime": 3.599056603773585,
  "claps": 0,
  "voters": 0
}